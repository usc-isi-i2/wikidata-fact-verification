{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "from random import seed\n",
    "\n",
    "seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_data = json.load(open('../data/unlabelled/train_fact_verification_support_data.json'))\n",
    "\n",
    "test_data_common = json.load(open('../data/unlabelled/gt_fact_verification_support_data.json'))\n",
    "test_data_only_dbp = json.load(open('../data/unlabelled/gt_only_dbp_support_data.json'))\n",
    "test_data = test_data_common + test_data_only_dbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total persons in test facts: 396\n"
     ]
    }
   ],
   "source": [
    "test_data_persons = set()\n",
    "for data in test_data:\n",
    "    for person in data['pair']:\n",
    "        test_data_persons.add(person)\n",
    "\n",
    "print(f'Total persons in test facts: {len(test_data_persons)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:04<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "train_facts = []\n",
    "train_facts_direct = []\n",
    "train_facts_only_negative_others = []\n",
    "train_facts_only_positive_others = []\n",
    "\n",
    "for data in tqdm(train_data):\n",
    "    person_one, person_two = data['pair']\n",
    "    if any((person in test_data_persons) for person in data['pair']):\n",
    "        continue\n",
    "\n",
    "    supports = sorted(data['supports'], key=lambda x: x['score'])\n",
    "    for i, support in enumerate(supports):\n",
    "        other_persons = []\n",
    "        pair_names = defaultdict(list)\n",
    "\n",
    "        evidence = support['content'].replace('\\n', ' ')\n",
    "        entities = token_classifier(evidence)\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity['entity_group'] != 'PER':\n",
    "                continue\n",
    "\n",
    "            person_name = entity['word']\n",
    "            if person_one in person_name or person_name in person_one:\n",
    "                pair_names[person_one].append(person_name)\n",
    "                continue\n",
    "\n",
    "            if person_two in person_name or person_name in person_two:\n",
    "                pair_names[person_two].append(person_name)\n",
    "                continue\n",
    "\n",
    "            other_persons.append(person_name)\n",
    "\n",
    "        person_one_names, person_two_names = pair_names[person_one], pair_names[person_two]\n",
    "        shuffle(person_one_names), shuffle(person_two_names), shuffle(other_persons)\n",
    "\n",
    "        if all(v for v in pair_names.values()):  # Add +ve entry for only most confident support\n",
    "            train_facts.append({'input': f'Is {person_one} married with {person_two}?\\n{evidence}', 'output': 'yes'})\n",
    "            train_facts.append({'input': f'Is {person_two} married with {person_one}?\\n{evidence}', 'output': 'yes'})\n",
    "\n",
    "            train_facts_direct.append({'input': f'Is {person_one} married with {person_two}?\\n{evidence}', 'output': 'yes'})\n",
    "            train_facts_direct.append({'input': f'Is {person_two} married with {person_one}?\\n{evidence}', 'output': 'yes'})\n",
    "\n",
    "            if person_one_names[0] != person_one or person_two_names[0] != person_two:\n",
    "                train_facts.append({'input': f'Is {person_one_names[0]} married with {person_two_names[0]}?\\n{evidence}', 'output': 'yes'})\n",
    "                train_facts.append({'input': f'Is {person_two_names[0]} married with {person_one_names[0]}?\\n{evidence}', 'output': 'yes'})\n",
    "\n",
    "                train_facts_only_positive_others.append({'input': f'Is {person_one_names[0]} married with {person_two_names[0]}?\\n{evidence}', 'output': 'yes'})\n",
    "                train_facts_only_positive_others.append({'input': f'Is {person_two_names[0]} married with {person_one_names[0]}?\\n{evidence}', 'output': 'yes'})\n",
    "\n",
    "        else:\n",
    "            train_facts.append({'input': f'Is {person_one} married with {person_two}?\\n{evidence}', 'output': 'no'})\n",
    "            train_facts.append({'input': f'Is {person_two} married with {person_one}?\\n{evidence}', 'output': 'no'})\n",
    "\n",
    "            train_facts_direct.append({'input': f'Is {person_one} married with {person_two}?\\n{evidence}', 'output': 'no'})\n",
    "            train_facts_direct.append({'input': f'Is {person_two} married with {person_one}?\\n{evidence}', 'output': 'no'})\n",
    "\n",
    "        if len(other_persons) >= 2:\n",
    "            train_facts.append({'input': f'Is {other_persons[0]} married with {other_persons[1]}?\\n{evidence}', 'output': 'no'})\n",
    "            train_facts.append({'input': f'Is {other_persons[1]} married with {other_persons[0]}?\\n{evidence}', 'output': 'no'})\n",
    "\n",
    "            train_facts_only_negative_others.append({'input': f'Is {other_persons[0]} married with {other_persons[1]}?\\n{evidence}', 'output': 'no'})\n",
    "            train_facts_only_negative_others.append({'input': f'Is {other_persons[1]} married with {other_persons[0]}?\\n{evidence}', 'output': 'no'})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "dataset_sizes = [500, 1000, 2000, 4000, 8000, 20000]\n",
    "datasets = {\n",
    "    'all': train_facts,\n",
    "    'direct': train_facts_direct,\n",
    "    'indirect_pos': train_facts_direct + train_facts_only_positive_others,\n",
    "    'indirect_neg': train_facts_direct + train_facts_only_negative_others\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/unifiedQA/train_all_500.json: pos_count:250 neg_count:250 total:500\n",
      "../data/unifiedQA/train_all_1000.json: pos_count:500 neg_count:500 total:1000\n",
      "../data/unifiedQA/train_all_2000.json: pos_count:1000 neg_count:1000 total:2000\n",
      "../data/unifiedQA/train_all_4000.json: pos_count:2000 neg_count:2000 total:4000\n",
      "../data/unifiedQA/train_all_8000.json: pos_count:4000 neg_count:4000 total:8000\n",
      "../data/unifiedQA/train_all_15144.json: pos_count:7572 neg_count:7572 total:15144\n",
      "../data/unifiedQA/train_direct_500.json: pos_count:250 neg_count:250 total:500\n",
      "../data/unifiedQA/train_direct_1000.json: pos_count:500 neg_count:500 total:1000\n",
      "../data/unifiedQA/train_direct_2000.json: pos_count:1000 neg_count:1000 total:2000\n",
      "../data/unifiedQA/train_direct_4000.json: pos_count:2000 neg_count:2000 total:4000\n",
      "../data/unifiedQA/train_direct_5208.json: pos_count:2604 neg_count:2604 total:5208\n",
      "../data/unifiedQA/train_indirect_pos_500.json: pos_count:250 neg_count:250 total:500\n",
      "../data/unifiedQA/train_indirect_pos_1000.json: pos_count:500 neg_count:500 total:1000\n",
      "../data/unifiedQA/train_indirect_pos_2000.json: pos_count:1000 neg_count:1000 total:2000\n",
      "../data/unifiedQA/train_indirect_pos_4000.json: pos_count:2000 neg_count:2000 total:4000\n",
      "../data/unifiedQA/train_indirect_pos_5208.json: pos_count:2604 neg_count:2604 total:5208\n",
      "../data/unifiedQA/train_indirect_neg_500.json: pos_count:250 neg_count:250 total:500\n",
      "../data/unifiedQA/train_indirect_neg_1000.json: pos_count:500 neg_count:500 total:1000\n",
      "../data/unifiedQA/train_indirect_neg_2000.json: pos_count:1000 neg_count:1000 total:2000\n",
      "../data/unifiedQA/train_indirect_neg_4000.json: pos_count:2000 neg_count:2000 total:4000\n",
      "../data/unifiedQA/train_indirect_neg_8000.json: pos_count:4000 neg_count:4000 total:8000\n",
      "../data/unifiedQA/train_indirect_neg_8520.json: pos_count:4260 neg_count:4260 total:8520\n"
     ]
    }
   ],
   "source": [
    "# Delete all training files from data/unifiedQA before creating new training files\n",
    "for name, dataset in datasets.items():\n",
    "    pos_facts = [fact for fact in dataset if fact['output'] == 'yes']\n",
    "    neg_facts = [fact for fact in dataset if fact['output'] == 'no']\n",
    "\n",
    "    for size in dataset_sizes:\n",
    "        shuffle(pos_facts), shuffle(neg_facts)\n",
    "        per_label_size = size // 2\n",
    "\n",
    "        pos_selection_size = per_label_size if per_label_size < len(pos_facts) else len(pos_facts)\n",
    "        neg_selection_size = per_label_size if per_label_size < len(neg_facts) else len(neg_facts)\n",
    "\n",
    "        label_selection_size = min(pos_selection_size, neg_selection_size)\n",
    "\n",
    "        pos_selection = pos_facts[:label_selection_size]\n",
    "        neg_selection = neg_facts[:label_selection_size]\n",
    "        selection = pos_selection + neg_selection\n",
    "\n",
    "        filename = f'../data/unifiedQA/train_{name}_{len(selection)}.json'\n",
    "        print(f'{filename}: pos_count:{len(pos_selection)} neg_count:{len(neg_selection)} total:{len(selection)}')\n",
    "\n",
    "        json.dump(selection, open(filename, 'w'))\n",
    "\n",
    "        if label_selection_size < per_label_size:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}