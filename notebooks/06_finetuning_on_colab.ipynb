{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "FineTune_unifiedQA_e3.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdooNBoCosGW",
    "outputId": "f464cada-5633-430e-91d3-91aa9861e200",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "Cloning into 'wikidata-fact-verification'...\n",
      "remote: Enumerating objects: 276, done.\u001B[K\n",
      "remote: Counting objects: 100% (276/276), done.\u001B[K\n",
      "remote: Compressing objects: 100% (155/155), done.\u001B[K\n",
      "remote: Total 276 (delta 168), reused 221 (delta 116), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (276/276), 5.44 MiB | 6.20 MiB/s, done.\n",
      "Resolving deltas: 100% (168/168), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece] > install_transformers.log\n",
    "!pip install accelerate > install_accelerator.log\n",
    "!apt install htop -y > install_htop.log\n",
    "# Github: Use personal access token with minimum access to work on colab. And set PAT expiry to 7 days for safety purpose.\n",
    "!git clone https://github.com/abhinav-kumar-thakur/wikidata-fact-verification.git\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd wikidata-fact-verification/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noHZ_KeyXx9F",
    "outputId": "80fd454f-d0dc-4401-f659-487e030764ae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/wikidata-fact-verification\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python train.py --model_size=large --train_dataset=train_direct_2000 --train_batch_size=8"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr48xXZ8X1iU",
    "outputId": "e11da65a-87d9-429d-f8bd-18dd07d5a832",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on device: cuda\n",
      "Downloading spiece.model: 100% 773k/773k [00:00<00:00, 57.4MB/s]\n",
      "Downloading special_tokens_map.json: 100% 1.74k/1.74k [00:00<00:00, 1.59MB/s]\n",
      "Downloading tokenizer_config.json: 100% 2.09k/2.09k [00:00<00:00, 1.94MB/s]\n",
      "Downloading config.json: 100% 1.33k/1.33k [00:00<00:00, 1.33MB/s]\n",
      "Downloading pytorch_model.bin: 100% 2.75G/2.75G [00:37<00:00, 79.2MB/s]\n",
      "--------------------------------------------------\n",
      "Run configurations: model=allenai/unifiedqa-v2-t5-large-1251000 train=./data/unifiedQA/train_direct_2000.json eval=./data/unifiedQA/test.json train_batch=8 eval_batch=16 epochs=20\n",
      "--------------------------------------------------\n",
      "Pre fine-tuning evaluations:\n",
      "Evaluating dataset: train\n",
      "  0% 0/125 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1207: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n",
      "100% 125/125 [02:00<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.7783251231527094, recall: 0.79, F1: 0.7841191066997518, accuracy: 0.7825\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:10<00:00,  1.23it/s]\n",
      "\n",
      "precision: 0.9411764705882353, recall: 0.8727272727272727, F1: 0.9056603773584905, accuracy: 0.9\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 0\n",
      "100% 250/250 [06:30<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 0: 73.00182895324542\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [01:59<00:00,  1.05it/s]\n",
      "\n",
      "precision: 0.9026465028355387, recall: 0.955, F1: 0.9280855199222545, accuracy: 0.926\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:11<00:00,  1.17it/s]\n",
      "\n",
      "precision: 0.9272727272727272, recall: 0.9272727272727272, F1: 0.9272727272727272, accuracy: 0.92\n",
      "Saving model with accuracy: 0.92 on eval at epoch 0\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 1\n",
      "100% 250/250 [06:31<00:00,  1.57s/it]\n",
      "\n",
      "Total loss: epoch 1: 30.15416105929762\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [01:59<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.9847405900305188, recall: 0.968, F1: 0.9762985375693394, accuracy: 0.9765\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:11<00:00,  1.16it/s]\n",
      "\n",
      "precision: 0.926605504587156, recall: 0.9181818181818182, F1: 0.9223744292237444, accuracy: 0.915\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 2\n",
      "100% 250/250 [06:30<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 2: 13.326548096981469\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [01:59<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.9930417495029821, recall: 0.999, F1: 0.9960119641076769, accuracy: 0.996\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:10<00:00,  1.19it/s]\n",
      "\n",
      "precision: 0.8974358974358975, recall: 0.9545454545454546, F1: 0.9251101321585904, accuracy: 0.915\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 3\n",
      "100% 250/250 [06:28<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 3: 7.528005409607674\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [02:00<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.9989327641408752, recall: 0.9369369369369369, F1: 0.9669421487603307, accuracy: 0.9635\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:10<00:00,  1.20it/s]\n",
      "\n",
      "precision: 0.87, recall: 0.7909090909090909, F1: 0.8285714285714286, accuracy: 0.82\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 4\n",
      "100% 250/250 [06:29<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 4: 10.957029948030765\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [02:00<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.9107468123861566, recall: 1.0, F1: 0.9532888465204957, accuracy: 0.951\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:11<00:00,  1.16it/s]\n",
      "\n",
      "precision: 0.8208955223880597, recall: 1.0, F1: 0.9016393442622952, accuracy: 0.88\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 5\n",
      "100% 250/250 [06:29<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 5: 4.517105187439675\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [01:59<00:00,  1.05it/s]\n",
      "\n",
      "precision: 0.9969450101832994, recall: 0.979, F1: 0.987891019172553, accuracy: 0.988\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      "100% 13/13 [00:10<00:00,  1.20it/s]\n",
      "\n",
      "precision: 0.9183673469387755, recall: 0.8181818181818182, F1: 0.8653846153846154, accuracy: 0.86\n",
      "--------------------------------------------------\n",
      "Starting training: epoch 6\n",
      "100% 250/250 [06:30<00:00,  1.56s/it]\n",
      "\n",
      "Total loss: epoch 6: 9.939243231688096\n",
      "----------\n",
      "Evaluating dataset: train\n",
      "100% 125/125 [02:00<00:00,  1.04it/s]\n",
      "\n",
      "precision: 0.9930348258706467, recall: 0.998, F1: 0.9955112219451371, accuracy: 0.9955\n",
      "----------\n",
      "Evaluating dataset: eval\n",
      " 77% 10/13 [00:08<00:02,  1.12it/s]"
     ]
    }
   ]
  }
 ]
}